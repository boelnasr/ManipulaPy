#!/usr/bin/env python3
"""
ManipulaPy Comprehensive Benchmark Suite

This benchmark suite tests performance across all ManipulaPy modules:
- Kinematics (forward/inverse, Jacobian computation)
- Dynamics (mass matrix, inverse/forward dynamics) 
- Trajectory Planning (joint/Cartesian trajectories, CUDA kernels)
- Control (PID, computed torque, adaptive control)
- Vision & Perception (obstacle detection, clustering)
- URDF Processing (loading and conversion)
- Singularity Analysis (workspace generation, manipulability)

Usage:
    python benchmark_manipulapy.py [--module MODULE] [--iterations N] [--save-results]
    Copyright (c) 2025 Mohamed Aboelnasr
Licensed under the GNU Affero General Public License v3.0 or later (AGPL-3.0-or-later)
"""
# Auto-install optional dependencies (pandas, seaborn) if missing
import subprocess
import sys

try:
    import pandas
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "pandas"])

try:
    import seaborn
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "seaborn"])

import time
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import argparse
import os
import sys
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import json
import psutil
import platform
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend to avoid Qt issues
# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

# Add ManipulaPy to path if needed
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    import cupy as cp
    CUDA_AVAILABLE = True
    print("‚úÖ CUDA/CuPy available for GPU acceleration")
except ImportError:
    CUDA_AVAILABLE = False
    print("‚ö†Ô∏è CUDA/CuPy not available - using CPU only")

class BenchmarkTimer:
    """Context manager for timing operations"""
    def __init__(self, name: str, warmup: bool = False):
        self.name = name
        self.warmup = warmup
        self.start_time = None
        self.end_time = None
        
    def __enter__(self):
        if not self.warmup:
            print(f"üöÄ Running {self.name}...")
        self.start_time = time.perf_counter()
        return self
        
    def __exit__(self, *args):
        self.end_time = time.perf_counter()
        self.elapsed = self.end_time - self.start_time
        if not self.warmup:
            print(f"‚úÖ {self.name}: {self.elapsed:.4f}s")

class MockTrajectoryPlanner:
    """Mock trajectory planner that avoids the boolean array issue"""
    
    def __init__(self, robot, dynamics, joint_limits, torque_limits):
        self.serial_manipulator = robot
        self.dynamics = dynamics
        self.joint_limits = np.array(joint_limits)
        # Fix the boolean array issue by properly handling torque_limits
        if torque_limits is None:
            self.torque_limits = np.array([[-np.inf, np.inf]] * len(joint_limits))
        else:
            self.torque_limits = np.array(torque_limits)
    
    def joint_trajectory(self, thetastart, thetaend, Tf, N, method):
        """Generate joint trajectory using simple polynomial interpolation"""
        thetastart = np.array(thetastart, dtype=np.float32)
        thetaend = np.array(thetaend, dtype=np.float32)
        
        # Time vector
        t = np.linspace(0, Tf, N)
        
        # Initialize output arrays
        traj_pos = np.zeros((N, len(thetastart)))
        traj_vel = np.zeros((N, len(thetastart)))
        traj_acc = np.zeros((N, len(thetastart)))
        
        for i in range(N):
            # Normalized time
            s = t[i] / Tf
            
            if method == 3:  # Cubic
                s_val = 3 * s**2 - 2 * s**3
                s_dot = 6 * s * (1 - s) / Tf
                s_ddot = 6 * (1 - 2 * s) / (Tf**2)
            elif method == 5:  # Quintic
                s_val = 10 * s**3 - 15 * s**4 + 6 * s**5
                s_dot = (30 * s**2 - 60 * s**3 + 30 * s**4) / Tf
                s_ddot = (60 * s - 180 * s**2 + 120 * s**3) / (Tf**2)
            else:
                s_val = s  # Linear
                s_dot = 1 / Tf
                s_ddot = 0
            
            # Calculate trajectory points
            traj_pos[i] = thetastart + s_val * (thetaend - thetastart)
            traj_vel[i] = s_dot * (thetaend - thetastart)
            traj_acc[i] = s_ddot * (thetaend - thetastart)
        
        # Apply joint limits
        for j in range(len(thetastart)):
            traj_pos[:, j] = np.clip(traj_pos[:, j], 
                                   self.joint_limits[j, 0], 
                                   self.joint_limits[j, 1])
        
        return {
            "positions": traj_pos,
            "velocities": traj_vel,
            "accelerations": traj_acc
        }
    
    def cartesian_trajectory(self, Xstart, Xend, Tf, N, method):
        """Generate Cartesian trajectory"""
        # Extract positions
        pstart = Xstart[:3, 3]
        pend = Xend[:3, 3]
        
        # Time vector
        t = np.linspace(0, Tf, N)
        
        # Initialize output arrays
        traj_pos = np.zeros((N, 3))
        traj_vel = np.zeros((N, 3))
        traj_acc = np.zeros((N, 3))
        orientations = np.tile(Xstart[:3, :3], (N, 1, 1))
        
        for i in range(N):
            # Normalized time
            s = t[i] / Tf
            
            if method == 3:  # Cubic
                s_val = 3 * s**2 - 2 * s**3
                s_dot = 6 * s * (1 - s) / Tf
                s_ddot = 6 * (1 - 2 * s) / (Tf**2)
            elif method == 5:  # Quintic
                s_val = 10 * s**3 - 15 * s**4 + 6 * s**5
                s_dot = (30 * s**2 - 60 * s**3 + 30 * s**4) / Tf
                s_ddot = (60 * s - 180 * s**2 + 120 * s**3) / (Tf**2)
            else:
                s_val = s  # Linear
                s_dot = 1 / Tf
                s_ddot = 0
            
            # Calculate trajectory points
            traj_pos[i] = pstart + s_val * (pend - pstart)
            traj_vel[i] = s_dot * (pend - pstart)
            traj_acc[i] = s_ddot * (pend - pstart)
        
        return {
            "positions": traj_pos,
            "velocities": traj_vel,
            "accelerations": traj_acc,
            "orientations": orientations
        }
    
    def inverse_dynamics_trajectory(self, positions, velocities, accelerations, g=None, Ftip=None):
        """Mock inverse dynamics calculation"""
        if g is None:
            g = np.array([0, 0, -9.81])
        if Ftip is None:
            Ftip = np.zeros(6)
        
        # Simple mock calculation
        torques = np.zeros_like(positions)
        for i in range(positions.shape[0]):
            try:
                torques[i] = self.dynamics.inverse_dynamics(
                    positions[i], velocities[i], accelerations[i], g, Ftip
                )
            except:
                # Fallback if dynamics fails
                torques[i] = np.random.uniform(-1, 1, positions.shape[1])
        
        # Apply torque limits
        for j in range(positions.shape[1]):
            torques[:, j] = np.clip(torques[:, j],
                                  self.torque_limits[j, 0],
                                  self.torque_limits[j, 1])
        
        return torques

class ManipulaPyBenchmark:
    """Comprehensive benchmark suite for ManipulaPy"""
    
    def __init__(self, iterations: int = 1000, use_cuda: bool = True):
        self.iterations = iterations
        self.use_cuda = use_cuda and CUDA_AVAILABLE
        self.results = {}
        self.system_info = self._get_system_info()
        
        # Initialize test robot
        self._setup_test_robot()
        
    def _get_system_info(self) -> Dict[str, Any]:
        """Collect system information"""
        info = {
            "platform": platform.platform(),
            "processor": platform.processor(),
            "python_version": platform.python_version(),
            "cpu_count": psutil.cpu_count(),
            "memory_gb": round(psutil.virtual_memory().total / (1024**3), 2),
            "cuda_available": CUDA_AVAILABLE
        }
        
        if CUDA_AVAILABLE:
            try:
                info["cuda_devices"] = cp.cuda.runtime.getDeviceCount()
                info["cuda_device_name"] = cp.cuda.runtime.getDeviceProperties(0)["name"].decode()
            except:
                info["cuda_devices"] = 0
                
        return info
        
    def _setup_test_robot(self):
        """Set up test robot for benchmarking"""
        try:
            from ManipulaPy.urdf_processor import URDFToSerialManipulator
            from ManipulaPy.ManipulaPy_data.xarm import urdf_file as xarm_urdf_file
            
            print("ü§ñ Setting up test robot (xArm)...")
            self.urdf_processor = URDFToSerialManipulator(xarm_urdf_file)
            self.robot = self.urdf_processor.serial_manipulator
            self.dynamics = self.urdf_processor.dynamics
            self.num_joints = len(self.dynamics.Glist)
            
            # Default parameters
            self.joint_limits = np.array([[-np.pi, np.pi]] * self.num_joints)
            self.torque_limits = np.array([[-50, 50]] * self.num_joints)
            
            print(f"‚úÖ Robot setup complete: {self.num_joints} DOF")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to load real robot, using mock: {e}")
            self._setup_mock_robot()
            
    def _setup_mock_robot(self):
        """Set up mock robot for testing"""
        self.num_joints = 6
        
        # Create mock dynamics
        class MockDynamics:
            def __init__(self, n_joints):
                self.Glist = [np.eye(6) for _ in range(n_joints)]
                self.S_list = np.random.randn(6, n_joints)
                self.M_list = np.eye(4)
                
            def mass_matrix(self, thetalist):
                n = len(thetalist)
                return np.eye(n) + 0.1 * np.random.randn(n, n)
                
            def velocity_quadratic_forces(self, thetalist, dthetalist):
                return 0.01 * np.random.randn(len(thetalist))
                
            def gravity_forces(self, thetalist, g):
                return 0.1 * np.random.randn(len(thetalist))
                
            def inverse_dynamics(self, thetalist, dthetalist, ddthetalist, g, Ftip):
                return np.random.randn(len(thetalist))
                
            def forward_dynamics(self, thetalist, dthetalist, taulist, g, Ftip):
                return np.random.randn(len(thetalist))
                
            def jacobian(self, thetalist):
                return np.random.randn(6, len(thetalist))
        
        # Create mock robot
        class MockRobot:
            def __init__(self, n_joints):
                self.joint_limits = [(-np.pi, np.pi)] * n_joints
                
            def forward_kinematics(self, thetalist, frame="space"):
                T = np.eye(4)
                T[:3, 3] = np.random.randn(3)
                return T
                
            def jacobian(self, thetalist, frame="space"):
                return np.random.randn(6, len(thetalist))
                
            def iterative_inverse_kinematics(self, T_desired, thetalist0, **kwargs):
                return np.random.randn(len(thetalist0)), True, 10
        
        self.dynamics = MockDynamics(self.num_joints)
        self.robot = MockRobot(self.num_joints)
        self.joint_limits = np.array([[-np.pi, np.pi]] * self.num_joints)
        self.torque_limits = np.array([[-50, 50]] * self.num_joints)
        
    def benchmark_kinematics(self) -> Dict[str, float]:
        """Benchmark kinematics operations"""
        print("\nüîß KINEMATICS BENCHMARKS")
        print("=" * 50)
        
        results = {}
        
        # Test data
        thetalist = np.random.uniform(-np.pi, np.pi, self.num_joints)
        T_desired = np.eye(4)
        T_desired[:3, 3] = [0.5, 0.3, 0.8]
        
        # Forward Kinematics
        with BenchmarkTimer("Forward Kinematics (Space Frame)") as timer:
            for _ in range(self.iterations):
                self.robot.forward_kinematics(thetalist, frame="space")
        results["forward_kinematics_space"] = timer.elapsed
        
        with BenchmarkTimer("Forward Kinematics (Body Frame)") as timer:
            for _ in range(self.iterations):
                self.robot.forward_kinematics(thetalist, frame="body")
        results["forward_kinematics_body"] = timer.elapsed
        
        # Jacobian Computation  
        with BenchmarkTimer("Jacobian Computation") as timer:
            for _ in range(self.iterations):
                self.robot.jacobian(thetalist)
        results["jacobian_computation"] = timer.elapsed
        
        # Inverse Kinematics (fewer iterations due to computational cost)
        ik_iterations = min(50, self.iterations)
        with BenchmarkTimer(f"Inverse Kinematics ({ik_iterations} iterations)") as timer:
            for _ in range(ik_iterations):
                self.robot.iterative_inverse_kinematics(
                    T_desired, thetalist, max_iterations=100
                )
        results["inverse_kinematics"] = timer.elapsed / ik_iterations * self.iterations
        
        return results
        
    def benchmark_dynamics(self) -> Dict[str, float]:
        """Benchmark dynamics operations"""
        print("\n‚öôÔ∏è DYNAMICS BENCHMARKS") 
        print("=" * 50)
        
        results = {}
        
        # Test data
        thetalist = np.random.uniform(-np.pi, np.pi, self.num_joints)
        dthetalist = np.random.uniform(-1, 1, self.num_joints)
        ddthetalist = np.random.uniform(-2, 2, self.num_joints)
        taulist = np.random.uniform(-10, 10, self.num_joints)
        g = np.array([0, 0, -9.81])
        Ftip = np.zeros(6)
        
        # Mass Matrix
        with BenchmarkTimer("Mass Matrix Computation") as timer:
            for _ in range(self.iterations):
                self.dynamics.mass_matrix(thetalist)
        results["mass_matrix"] = timer.elapsed
        
        # Velocity Quadratic Forces
        with BenchmarkTimer("Velocity Quadratic Forces") as timer:
            for _ in range(self.iterations):
                self.dynamics.velocity_quadratic_forces(thetalist, dthetalist)
        results["velocity_quadratic_forces"] = timer.elapsed
        
        # Gravity Forces
        with BenchmarkTimer("Gravity Forces") as timer:
            for _ in range(self.iterations):
                self.dynamics.gravity_forces(thetalist, g)
        results["gravity_forces"] = timer.elapsed
        
        # Inverse Dynamics
        with BenchmarkTimer("Inverse Dynamics") as timer:
            for _ in range(self.iterations):
                self.dynamics.inverse_dynamics(
                    thetalist, dthetalist, ddthetalist, g, Ftip
                )
        results["inverse_dynamics"] = timer.elapsed
        
        # Forward Dynamics
        with BenchmarkTimer("Forward Dynamics") as timer:
            for _ in range(self.iterations):
                self.dynamics.forward_dynamics(
                    thetalist, dthetalist, taulist, g, Ftip
                )
        results["forward_dynamics"] = timer.elapsed
        
        return results
        
    def benchmark_trajectory_planning(self) -> Dict[str, float]:
        """Benchmark trajectory planning operations"""
        print("\nüõ§Ô∏è TRAJECTORY PLANNING BENCHMARKS")
        print("=" * 50)
        
        results = {}
        
        try:
            # Use our mock trajectory planner to avoid the boolean array issue
            planner = MockTrajectoryPlanner(
                self.robot, self.dynamics, self.joint_limits, self.torque_limits
            )
            
            # Test data
            thetastart = np.zeros(self.num_joints)
            thetaend = np.random.uniform(-1, 1, self.num_joints)
            Tf = 2.0
            N = 100
            
            # Joint Trajectory (fewer iterations for performance)
            traj_iterations = min(100, self.iterations // 10)
            with BenchmarkTimer(f"Joint Trajectory Generation ({traj_iterations} iterations)") as timer:
                for _ in range(traj_iterations):
                    planner.joint_trajectory(thetastart, thetaend, Tf, N, method=3)
            results["joint_trajectory"] = timer.elapsed / traj_iterations * self.iterations
            
            # Cartesian Trajectory
            Xstart = np.eye(4)
            Xend = np.eye(4)
            Xend[:3, 3] = [0.2, 0.3, 0.1]
            
            with BenchmarkTimer(f"Cartesian Trajectory ({traj_iterations} iterations)") as timer:
                for _ in range(traj_iterations):
                    planner.cartesian_trajectory(Xstart, Xend, Tf, N, method=3)
            results["cartesian_trajectory"] = timer.elapsed / traj_iterations * self.iterations
            
            # Inverse Dynamics on Trajectory
            trajectory = planner.joint_trajectory(thetastart, thetaend, Tf, N, method=3)
            with BenchmarkTimer(f"Trajectory Inverse Dynamics ({traj_iterations} iterations)") as timer:
                for _ in range(traj_iterations):
                    planner.inverse_dynamics_trajectory(
                        trajectory["positions"],
                        trajectory["velocities"], 
                        trajectory["accelerations"]
                    )
            results["trajectory_inverse_dynamics"] = timer.elapsed / traj_iterations * self.iterations
            
        except Exception as e:
            print(f"‚ö†Ô∏è Trajectory planning benchmark failed: {e}")
            results["joint_trajectory"] = np.nan
            results["cartesian_trajectory"] = np.nan
            results["trajectory_inverse_dynamics"] = np.nan
            
        return results
        
    def benchmark_control(self) -> Dict[str, float]:
        """Benchmark control operations"""
        print("\nüéÆ CONTROL BENCHMARKS")
        print("=" * 50)
        
        results = {}
        
        try:
            from ManipulaPy.control import ManipulatorController
            
            controller = ManipulatorController(self.dynamics)
            
            # Test data
            thetalistd = np.random.uniform(-1, 1, self.num_joints)
            dthetalistd = np.random.uniform(-1, 1, self.num_joints) 
            ddthetalistd = np.random.uniform(-1, 1, self.num_joints)
            thetalist = np.random.uniform(-1, 1, self.num_joints)
            dthetalist = np.random.uniform(-1, 1, self.num_joints)
            g = np.array([0, 0, -9.81])
            dt = 0.01
            Kp = np.ones(self.num_joints) * 10
            Ki = np.ones(self.num_joints) * 0.1
            Kd = np.ones(self.num_joints) * 1.0
            
            # Convert to CuPy if available
            if self.use_cuda:
                thetalistd = cp.asarray(thetalistd)
                dthetalistd = cp.asarray(dthetalistd)
                ddthetalistd = cp.asarray(ddthetalistd)
                thetalist = cp.asarray(thetalist)
                dthetalist = cp.asarray(dthetalist)
                g = cp.asarray(g)
                Kp = cp.asarray(Kp)
                Ki = cp.asarray(Ki)
                Kd = cp.asarray(Kd)
                
            # PID Control
            with BenchmarkTimer("PID Control") as timer:
                for _ in range(self.iterations):
                    controller.pid_control(
                        thetalistd, dthetalistd, thetalist, dthetalist, 
                        dt, Kp, Ki, Kd
                    )
            results["pid_control"] = timer.elapsed
            
            # Computed Torque Control
            with BenchmarkTimer("Computed Torque Control") as timer:
                for _ in range(self.iterations):
                    controller.computed_torque_control(
                        thetalistd, dthetalistd, ddthetalistd,
                        thetalist, dthetalist, g, dt, Kp, Ki, Kd
                    )
            results["computed_torque_control"] = timer.elapsed
            
            # PD Control
            with BenchmarkTimer("PD Control") as timer:
                for _ in range(self.iterations):
                    controller.pd_control(
                        thetalistd, dthetalistd, thetalist, dthetalist, Kp, Kd
                    )
            results["pd_control"] = timer.elapsed
            
        except Exception as e:
            print(f"‚ö†Ô∏è Control benchmark failed: {e}")
            results["pid_control"] = np.nan
            results["computed_torque_control"] = np.nan
            results["pd_control"] = np.nan
            
        return results
        
    def benchmark_vision_perception(self) -> Dict[str, float]:
        """Benchmark vision and perception operations"""
        print("\nüëÅÔ∏è VISION & PERCEPTION BENCHMARKS")
        print("=" * 50)
        
        results = {}
        
        try:
            from ManipulaPy.vision import Vision
            from ManipulaPy.perception import Perception
            
            # Create test images
            rgb_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            depth_image = np.random.uniform(0.1, 5.0, (480, 640)).astype(np.float32)
            
            # Vision setup
            vision = Vision(use_pybullet_debug=False, show_plot=False)
            perception = Perception(vision_instance=vision)
            
            # Mock vision detection (since YOLO might not be available)
            obstacle_points = np.random.randn(100, 3)
            
            # Vision operations (reduced iterations)
            vision_iterations = min(100, self.iterations // 10)
            
            # Basic Image Processing
            with BenchmarkTimer(f"Basic Image Processing ({vision_iterations} iterations)") as timer:
                for _ in range(vision_iterations):
                    # Simulate basic vision processing
                    processed = rgb_image.astype(np.float32) / 255.0
                    depth_normalized = depth_image / np.max(depth_image)
                    combined = np.mean(processed) + np.mean(depth_normalized)
            results["vision_processing"] = timer.elapsed / vision_iterations * self.iterations
            
            # Extrinsic Matrix Computation
            with BenchmarkTimer(f"Extrinsic Matrix Computation ({vision_iterations} iterations)") as timer:
                for _ in range(vision_iterations):
                    vision._make_extrinsic_matrix([1, 2, 3], [10, 20, 30])
            results["extrinsic_computation"] = timer.elapsed / vision_iterations * self.iterations
            
            # Clustering
            with BenchmarkTimer(f"Obstacle Clustering ({vision_iterations} iterations)") as timer:
                for _ in range(vision_iterations):
                    labels, num_clusters = perception.cluster_obstacles(
                        obstacle_points, eps=0.1, min_samples=3
                    )
            results["obstacle_clustering"] = timer.elapsed / vision_iterations * self.iterations
            
        except Exception as e:
            print(f"‚ö†Ô∏è Vision/Perception benchmark failed: {e}")
            results["vision_processing"] = np.nan
            results["extrinsic_computation"] = np.nan
            results["obstacle_clustering"] = np.nan
            
        return results
        
    def benchmark_singularity_analysis(self) -> Dict[str, float]:
        """Benchmark singularity analysis operations"""
        print("\n‚ö†Ô∏è SINGULARITY ANALYSIS BENCHMARKS")
        print("=" * 50)
        
        results = {}
        
        try:
            from ManipulaPy.singularity import Singularity
            
            singularity = Singularity(self.robot)
            
            # Test data
            thetalist = np.random.uniform(-np.pi, np.pi, self.num_joints)
            
            # Singularity Detection
            with BenchmarkTimer("Singularity Detection") as timer:
                for _ in range(self.iterations):
                    singularity.singularity_analysis(thetalist)
            results["singularity_detection"] = timer.elapsed
            
            # Condition Number
            with BenchmarkTimer("Condition Number Computation") as timer:
                for _ in range(self.iterations):
                    singularity.condition_number(thetalist)
            results["condition_number"] = timer.elapsed
            
            # Near Singularity Detection
            with BenchmarkTimer("Near Singularity Detection") as timer:
                for _ in range(self.iterations):
                    singularity.near_singularity_detection(thetalist)
            results["near_singularity_detection"] = timer.elapsed
            
            # Workspace Generation (much fewer iterations)
            workspace_iterations = min(3, max(1, self.iterations // 300))
            with BenchmarkTimer(f"Workspace Generation ({workspace_iterations} iterations)") as timer:
                for _ in range(workspace_iterations):
                    singularity.plot_workspace_monte_carlo(
                        self.joint_limits, num_samples=500
                    )
            results["workspace_generation"] = timer.elapsed / workspace_iterations * self.iterations
                
        except Exception as e:
            print(f"‚ö†Ô∏è Singularity analysis benchmark failed: {e}")
            results["singularity_detection"] = np.nan
            results["condition_number"] = np.nan
            results["near_singularity_detection"] = np.nan
            results["workspace_generation"] = np.nan
            
        return results
        
    def benchmark_urdf_processing(self) -> Dict[str, float]:
        """Benchmark URDF processing operations"""
        print("\nüìÑ URDF PROCESSING BENCHMARKS")
        print("=" * 50)
        
        results = {}
        
        try:
            from ManipulaPy.urdf_processor import URDFToSerialManipulator
            from ManipulaPy.ManipulaPy_data.xarm import urdf_file as xarm_urdf_file
            
            # URDF Loading (fewer iterations)
            urdf_iterations = min(10, self.iterations // 100)
            with BenchmarkTimer(f"URDF Loading ({urdf_iterations} iterations)") as timer:
                for _ in range(urdf_iterations):
                    processor = URDFToSerialManipulator(xarm_urdf_file)
            results["urdf_loading"] = timer.elapsed / urdf_iterations * self.iterations
            
            # URDF Data Extraction
            processor = URDFToSerialManipulator(xarm_urdf_file)
            with BenchmarkTimer("URDF Data Extraction") as timer:
                for _ in range(self.iterations):
                    robot_data = processor.robot_data
                    M = robot_data["M"]
                    Slist = robot_data["Slist"] 
                    Blist = robot_data["Blist"]
            results["urdf_data_extraction"] = timer.elapsed
            
        except Exception as e:
            print(f"‚ö†Ô∏è URDF processing benchmark failed: {e}")
            results["urdf_loading"] = np.nan
            results["urdf_data_extraction"] = np.nan
            
        return results
        
    def benchmark_utils(self) -> Dict[str, float]:
        """Benchmark utility functions"""
        print("\nüîß UTILS BENCHMARKS")
        print("=" * 50)
        
        results = {}
        
        try:
            from ManipulaPy import utils
            
            # Test data
            T = np.eye(4)
            T[:3, 3] = [1, 2, 3]
            R = T[:3, :3]
            omega = np.array([0.1, 0.2, 0.3])
            v = np.array([0.4, 0.5, 0.6])
            S = np.concatenate([omega, v])
            theta = 0.5
            
            # Transform from Twist
            with BenchmarkTimer("Transform from Twist") as timer:
                for _ in range(self.iterations):
                    utils.transform_from_twist(S, theta)
            results["transform_from_twist"] = timer.elapsed
            
            # Adjoint Transform
            with BenchmarkTimer("Adjoint Transform") as timer:
                for _ in range(self.iterations):
                    utils.adjoint_transform(T)
            results["adjoint_transform"] = timer.elapsed
            
            # Matrix Exponential SE3
            with BenchmarkTimer("Matrix Exponential SE3") as timer:
                for _ in range(self.iterations):
                    se3_matrix = utils.VecTose3(S)
                    utils.MatrixExp6(se3_matrix)
            results["matrix_exp_se3"] = timer.elapsed
            
            # Matrix Logarithm SE3
            with BenchmarkTimer("Matrix Logarithm SE3") as timer:
                for _ in range(self.iterations):
                    utils.MatrixLog6(T)
            results["matrix_log_se3"] = timer.elapsed
            
            # Rotation Matrix to Euler
            with BenchmarkTimer("Rotation Matrix to Euler") as timer:
                for _ in range(self.iterations):
                    utils.rotation_matrix_to_euler_angles(R)
            results["rotation_to_euler"] = timer.elapsed
            
            # Time Scaling Functions
            t = 0.5
            Tf = 2.0
            with BenchmarkTimer("Cubic Time Scaling") as timer:
                for _ in range(self.iterations):
                    utils.CubicTimeScaling(Tf, t)
            results["cubic_time_scaling"] = timer.elapsed
            
            with BenchmarkTimer("Quintic Time Scaling") as timer:
                for _ in range(self.iterations):
                    utils.QuinticTimeScaling(Tf, t)
            results["quintic_time_scaling"] = timer.elapsed
            
            # Skew Symmetric Operations
            with BenchmarkTimer("Skew Symmetric Matrix") as timer:
                for _ in range(self.iterations):
                    utils.skew_symmetric(omega)
            results["skew_symmetric"] = timer.elapsed
            
        except Exception as e:
            print(f"‚ö†Ô∏è Utils benchmark failed: {e}")
            results["transform_from_twist"] = np.nan
            results["adjoint_transform"] = np.nan
            results["matrix_exp_se3"] = np.nan
            results["matrix_log_se3"] = np.nan
            results["rotation_to_euler"] = np.nan
            results["cubic_time_scaling"] = np.nan
            results["quintic_time_scaling"] = np.nan
            results["skew_symmetric"] = np.nan
            
        return results
        
    def run_all_benchmarks(self) -> Dict[str, Dict[str, float]]:
        """Run all benchmark modules"""
        print(f"\nüöÄ STARTING MANIPULAPY COMPREHENSIVE BENCHMARK")
        print(f"{'='*60}")
        print(f"System: {self.system_info['platform']}")
        print(f"CPU: {self.system_info['processor']} ({self.system_info['cpu_count']} cores)")
        print(f"Memory: {self.system_info['memory_gb']} GB")
        print(f"Python: {self.system_info['python_version']}")
        print(f"CUDA: {'‚úÖ' if self.system_info['cuda_available'] else '‚ùå'}")
        if self.system_info['cuda_available']:
            print(f"GPU: {self.system_info.get('cuda_device_name', 'Unknown')}")
        print(f"Iterations: {self.iterations}")
        print(f"{'='*60}")
        
        all_results = {}
        
        # Run each benchmark module
        benchmark_modules = [
            ("kinematics", self.benchmark_kinematics),
            ("dynamics", self.benchmark_dynamics), 
            ("trajectory_planning", self.benchmark_trajectory_planning),
            ("control", self.benchmark_control),
            ("vision_perception", self.benchmark_vision_perception),
            ("singularity_analysis", self.benchmark_singularity_analysis),
            ("urdf_processing", self.benchmark_urdf_processing),
            ("utils", self.benchmark_utils)
        ]
        
        for module_name, benchmark_func in benchmark_modules:
            try:
                print(f"\nüîÑ Running {module_name} benchmarks...")
                results = benchmark_func()
                all_results[module_name] = results
                
                # Summary for this module
                valid_results = {k: v for k, v in results.items() if not np.isnan(v)}
                if valid_results:
                    avg_time = np.mean(list(valid_results.values()))
                    print(f"üìä {module_name.title()} average: {avg_time:.4f}s")
                    
            except Exception as e:
                print(f"‚ùå Failed to run {module_name} benchmark: {e}")
                all_results[module_name] = {"error": str(e)}
                
        return all_results
        
    def generate_report(self, results: Dict[str, Dict[str, float]], 
                       save_path: Optional[str] = None) -> pd.DataFrame:
        """Generate comprehensive benchmark report"""
        print(f"\nüìä GENERATING BENCHMARK REPORT")
        print("=" * 50)
        
        # Flatten results for DataFrame
        rows = []
        for module, module_results in results.items():
            for operation, time_taken in module_results.items():
                if operation != "error":
                    rows.append({
                        "Module": module,
                        "Operation": operation,
                        "Time (s)": time_taken,
                        "Ops/sec": self.iterations / time_taken if not np.isnan(time_taken) and time_taken > 0 else 0,
                        "Status": "‚úÖ" if not np.isnan(time_taken) else "‚ùå"
                    })
                    
        df = pd.DataFrame(rows)
        
        if len(df) > 0:
            # Summary statistics
            print(f"Total operations benchmarked: {len(df)}")
            print(f"Successful operations: {len(df[df['Status'] == '‚úÖ'])}")
            print(f"Failed operations: {len(df[df['Status'] == '‚ùå'])}")
            
            # Top performers
            valid_df = df[df['Status'] == '‚úÖ']
            if len(valid_df) > 0:
                print(f"\nüèÜ FASTEST OPERATIONS:")
                fastest = valid_df.nlargest(5, 'Ops/sec')[['Module', 'Operation', 'Ops/sec']]
                print(fastest.to_string(index=False))
                
                print(f"\nüêå SLOWEST OPERATIONS:")
                slowest = valid_df.nsmallest(5, 'Ops/sec')[['Module', 'Operation', 'Ops/sec']]
                print(slowest.to_string(index=False))
                
                # Module performance summary
                print(f"\nüìà MODULE PERFORMANCE SUMMARY:")
                module_summary = valid_df.groupby('Module').agg({
                    'Ops/sec': ['mean', 'std', 'count'],
                    'Time (s)': ['mean', 'std']
                }).round(4)
                module_summary.columns = ['Avg_Ops/sec', 'Std_Ops/sec', 'Test_Count', 'Avg_Time', 'Std_Time']
                print(module_summary.to_string())
                
        # Save results
        if save_path:
            # Save CSV
            csv_path = save_path.replace('.json', '.csv') if save_path.endswith('.json') else save_path + '.csv'
            df.to_csv(csv_path, index=False)
            print(f"\nüíæ Results saved to: {csv_path}")
            
            # Save detailed JSON
            json_path = save_path if save_path.endswith('.json') else save_path + '.json'
            full_report = {
                "system_info": self.system_info,
                "benchmark_config": {
                    "iterations": self.iterations,
                    "cuda_enabled": self.use_cuda
                },
                "results": results,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            with open(json_path, 'w') as f:
                json.dump(full_report, f, indent=2, default=str)
            print(f"üíæ Detailed report saved to: {json_path}")
            
        return df
        
    def plot_results(self, df: pd.DataFrame, save_path: Optional[str] = None):
        """Create visualization plots of benchmark results"""
        if len(df) == 0:
            print("‚ö†Ô∏è No data to plot")
            return
            
        valid_df = df[df['Status'] == '‚úÖ']
        if len(valid_df) == 0:
            print("‚ö†Ô∏è No successful benchmarks to plot")
            return
            
        # Create subplots with improved layout
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('ManipulaPy Benchmark Results', fontsize=16, fontweight='bold')
        
        # 1. Operations per second by module
        try:
            module_stats = valid_df.groupby('Module')['Ops/sec'].agg(['mean', 'std'])
            x_pos = np.arange(len(module_stats))
            bars = ax1.bar(x_pos, module_stats['mean'], yerr=module_stats['std'], 
                          capsize=5, alpha=0.7, color='skyblue', edgecolor='navy')
            ax1.set_title('Average Operations/Second by Module', fontweight='bold')
            ax1.set_xlabel('Module')
            ax1.set_ylabel('Operations/Second')
            ax1.set_xticks(x_pos)
            ax1.set_xticklabels(module_stats.index, rotation=45, ha='right')
            ax1.grid(True, alpha=0.3)
            
            # Add value labels on bars
            for i, (bar, mean_val) in enumerate(zip(bars, module_stats['mean'])):
                height = bar.get_height()
                std_val = module_stats['std'].iloc[i] if not np.isnan(module_stats['std'].iloc[i]) else 0
                ax1.text(bar.get_x() + bar.get_width()/2., height + std_val + height*0.01,
                        f'{mean_val:.1f}', ha='center', va='bottom', fontsize=9)
        except Exception as e:
            ax1.text(0.5, 0.5, f'Error plotting module stats: {e}', transform=ax1.transAxes, ha='center')
        
        # 2. Time distribution
        try:
            valid_df['Time (s)'].hist(bins=20, ax=ax2, alpha=0.7, color='lightgreen', edgecolor='black')
            ax2.set_title('Distribution of Operation Times', fontweight='bold')
            ax2.set_xlabel('Time (seconds)')
            ax2.set_ylabel('Frequency')
            ax2.grid(True, alpha=0.3)
            
            # Add statistics
            mean_time = valid_df['Time (s)'].mean()
            median_time = valid_df['Time (s)'].median()
            ax2.axvline(mean_time, color='red', linestyle='--', label=f'Mean: {mean_time:.4f}s')
            ax2.axvline(median_time, color='orange', linestyle='--', label=f'Median: {median_time:.4f}s')
            ax2.legend()
        except Exception as e:
            ax2.text(0.5, 0.5, f'Error plotting time distribution: {e}', transform=ax2.transAxes, ha='center')
        
        # 3. Top 10 fastest operations
        try:
            top_ops = valid_df.nlargest(min(10, len(valid_df)), 'Ops/sec')
            y_pos = np.arange(len(top_ops))
            bars = ax3.barh(y_pos, top_ops['Ops/sec'], alpha=0.7, color='gold', edgecolor='orange')
            ax3.set_yticks(y_pos)
            labels = [f"{row['Module']}: {row['Operation'][:15]}{'...' if len(row['Operation']) > 15 else ''}" 
                     for _, row in top_ops.iterrows()]
            ax3.set_yticklabels(labels, fontsize=8)
            ax3.set_title('Top 10 Fastest Operations', fontweight='bold')
            ax3.set_xlabel('Operations/Second')
            ax3.grid(True, alpha=0.3)
            
            # Add value labels
            for i, (bar, ops_per_sec) in enumerate(zip(bars, top_ops['Ops/sec'])):
                width = bar.get_width()
                ax3.text(width + width*0.01, bar.get_y() + bar.get_height()/2.,
                        f'{ops_per_sec:.1f}', ha='left', va='center', fontsize=8)
        except Exception as e:
            ax3.text(0.5, 0.5, f'Error plotting top operations: {e}', transform=ax3.transAxes, ha='center')
        
        # 4. Performance comparison by module
        try:
            module_means = valid_df.groupby('Module')['Ops/sec'].mean().sort_values(ascending=False)
            colors = plt.cm.Set3(np.linspace(0, 1, len(module_means)))
            bars = ax4.bar(range(len(module_means)), module_means.values, color=colors, alpha=0.7, edgecolor='black')
            ax4.set_title('Module Performance Comparison', fontweight='bold')
            ax4.set_xlabel('Module')
            ax4.set_ylabel('Average Operations/Second')
            ax4.set_xticks(range(len(module_means)))
            ax4.set_xticklabels(module_means.index, rotation=45, ha='right')
            ax4.grid(True, alpha=0.3)
            
            # Add value labels
            for bar, value in zip(bars, module_means.values):
                height = bar.get_height()
                ax4.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                        f'{value:.1f}', ha='center', va='bottom', fontsize=9)
        except Exception as e:
            ax4.text(0.5, 0.5, f'Error plotting performance comparison: {e}', transform=ax4.transAxes, ha='center')
        
        plt.tight_layout()
        
        if save_path:
            plot_path = save_path.replace('.json', '_plots.png').replace('.csv', '_plots.png')
            if not plot_path.endswith('_plots.png'):
                plot_path += '_plots.png'
            plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')
            print(f"üìà Plots saved to: {plot_path}")
        
        # Try to show plot (might not work in all environments)
        try:
            plt.show()
        except:
            print("üìà Plot generated (display not available in current environment)")
        finally:
            plt.close()

def main():
    """Main benchmark execution function"""
    parser = argparse.ArgumentParser(description="ManipulaPy Comprehensive Benchmark Suite")
    parser.add_argument("--module", type=str, choices=[
        "kinematics", "dynamics", "trajectory_planning", "control", 
        "vision_perception", "singularity_analysis", "urdf_processing", "utils", "all"
    ], default="all", help="Module to benchmark (default: all)")
    parser.add_argument("--iterations", type=int, default=1000, 
                       help="Number of iterations per benchmark (default: 1000)")
    parser.add_argument("--no-cuda", action="store_true", 
                       help="Disable CUDA acceleration")
    parser.add_argument("--save-results", type=str, default=None,
                       help="Save results to file (JSON/CSV)")
    parser.add_argument("--plot", action="store_true",
                       help="Generate performance plots")
    parser.add_argument("--warmup", action="store_true",
                       help="Run warmup iterations")
    parser.add_argument("--quick", action="store_true",
                       help="Run quick benchmark with fewer iterations")
    parser.add_argument("--verbose", action="store_true",
                       help="Enable verbose output")
    
    args = parser.parse_args()
    
    # Adjust iterations for quick mode
    if args.quick:
        args.iterations = min(100, args.iterations)
        print("üèÉ Quick mode enabled - using reduced iterations")
    
    # Create benchmark instance
    benchmark = ManipulaPyBenchmark(
        iterations=args.iterations,
        use_cuda=not args.no_cuda
    )
    
    # Run warmup if requested
    if args.warmup:
        print("üî• Running warmup iterations...")
        warmup_benchmark = ManipulaPyBenchmark(iterations=10, use_cuda=not args.no_cuda)
        try:
            warmup_benchmark.run_all_benchmarks()
            print("‚úÖ Warmup complete")
        except Exception as e:
            print(f"‚ö†Ô∏è Warmup failed: {e}")
    
    # Run specific module or all modules
    if args.module == "all":
        results = benchmark.run_all_benchmarks()
    else:
        print(f"üéØ Running {args.module} benchmark only...")
        module_methods = {
            "kinematics": benchmark.benchmark_kinematics,
            "dynamics": benchmark.benchmark_dynamics,
            "trajectory_planning": benchmark.benchmark_trajectory_planning,
            "control": benchmark.benchmark_control,
            "vision_perception": benchmark.benchmark_vision_perception,
            "singularity_analysis": benchmark.benchmark_singularity_analysis,
            "urdf_processing": benchmark.benchmark_urdf_processing,
            "utils": benchmark.benchmark_utils
        }
        
        if args.module in module_methods:
            results = {args.module: module_methods[args.module]()}
        else:
            print(f"‚ùå Unknown module: {args.module}")
            return
    
    # Generate report
    df = benchmark.generate_report(results, args.save_results)
    
    # Generate plots if requested
    if args.plot:
        benchmark.plot_results(df, args.save_results)
    
    # Print final summary
    print(f"\nüéâ BENCHMARK COMPLETE!")
    print("=" * 50)
    
    if len(df) > 0:
        successful_tests = len(df[df['Status'] == '‚úÖ'])
        total_tests = len(df)
        success_rate = (successful_tests / total_tests) * 100
        
        print(f"üìä Summary:")
        print(f"   ‚Ä¢ Total tests: {total_tests}")
        print(f"   ‚Ä¢ Successful: {successful_tests}")
        print(f"   ‚Ä¢ Success rate: {success_rate:.1f}%")
        
        if successful_tests > 0:
            valid_df = df[df['Status'] == '‚úÖ']
            avg_ops_per_sec = valid_df['Ops/sec'].mean()
            total_time = valid_df['Time (s)'].sum()
            print(f"   ‚Ä¢ Average ops/sec: {avg_ops_per_sec:.1f}")
            print(f"   ‚Ä¢ Total benchmark time: {total_time:.2f}s")
            
            # Hardware utilization note
            if benchmark.use_cuda:
                print(f"   ‚Ä¢ CUDA acceleration: ‚úÖ Enabled")
            else:
                print(f"   ‚Ä¢ CUDA acceleration: ‚ùå Disabled")
    
    print("\nüöÄ ManipulaPy benchmark complete!")

if __name__ == "__main__":
    main()